\section{Large Sample Properties}
Occasions we apply the large sample properties include the normality assumption on residuals may be too strong, there is a large amount of data and need to use other estimators and properties, or using techniques that rely on large sample theory.

It relies on two core theories: \textbf{\textit{Law of Large Numbers, Central Limit Theorem}}.

\begin{definition}
    Consider an estimator $\hat{\theta}$, it \textbf{\textit{converges in probability}} to $\theta$m, when as $n \to \infty$, the probability of any difference between the estimator and the actual value goes to $0$.
    $$\lim_{n \to \infty} P(|\hat{\theta} - \theta| > 0) = 0 \iff \hat{\theta} \to_p \theta$$
\end{definition}
This represents consistency, a weaker condition.

\begin{theorem}
    The \textbf{\textit{Law of Large Numbers}} states that for a random variable $X_i$, we have
    $$\hat{E}(X_i) \to_p E(X_i)$$
\end{theorem}
By this theorem, we can also demonstrate that $\hat{\beta_j}$ is consistent, the weak exogeneity condition.

\begin{definition}
    Consider an estimator $\hat{\theta}$, and $F$ being a CDF, we say it \textbf{\textit{converges in distribution}} to $F$ when
    $$\forall x, \lim_{n \to \infty} P(\hat{\theta} \le x) = F(x)$$
    we write $\hat{\theta} \sim_a F$
\end{definition}

This brings us to
\begin{theorem}
    The \textbf{\textit{Central Limit Theorem}} states that if a random variable $X_i$ has mean of $E(X_i)$ and variance $\sigma_X^2$, then
    $$\frac{\hat{E}(X_i) - E(X_i)}{\frac{1}{\sqrt{n}} \sqrt{E(X_i^2) - E(X_i)^2}} \sim N(0, 1)$$
\end{theorem}
as long as $E(\epsilon_i | X_i) = 0$, with no need to assume it being normal. This is equivalent to
$$\frac{\sqrt{n}(\hat{E}(\cdot) - E(\cdot))}{\sqrt{V(\cdot)}} \sim_a N(0, 1)$$

Thus, if $n \to \infty$, as long as $E(\epsilon_i | X_i) = 0$, then the estimates are consistent; furthermore, if the residuals are also normally distributed, the estimates remain efficient (error is homoskedastic).

\newpage