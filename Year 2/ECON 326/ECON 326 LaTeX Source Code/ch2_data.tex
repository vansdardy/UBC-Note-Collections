\section{Data}
\begin{definition}
    \textbf{\textit{Data or datasets}} are information used to answer econometric questions in interest. Specifically, econometric data are data generated by collective or individual choices and decisions.
\end{definition}

Most data in econometrics are \textbf{non-experimental}, or specifically observational, where researchers are not actively manipulating the population, in contrast to \textbf{experimental} data which are generated with an \textit{intervention}. \textbf{Quasi-experimental} data are non-experimental but can be \textit{imagined} as experimental.

Data can be good or bad, but bad data does not mean erroneous data. Bad data usually refers to data that are not suitable for the research, which means the data is misrepresentative of the population, the size may be too small, or it mismatches the structure of the population. We need \textbf{meta-analyses} to ensure validity externally.

\subsection{Structure of Data}
Data are usually of three structures: cross-sectional, time series, or panel.

Cross-section data are a \textbf{snapshot of population}, where time is usually fixed, and there are a number of independencies. The observations are focused on the independencies $(i)$.

Time series data are generated by observations on \textbf{time}, with only one independency. We observe how the independencies change with respect to time.

Combined together, they form panel data or longitudinal data, where there are both a number of independencies and a period of time. They can be balanced or unbalanced.

\subsection{Data and Models}
Models are designed to fit certain data.

Predictions from models are statements about the \textbf{\textit{population}}. However, we cannot directly observe the population. Therefore, we rely on \textbf{\textit{samples}} collected from the population (\textbf{sampling process}) to make estimates about the population. Tandem to other properties of the data, we refer this as the \textbf{data-generating process}.

Observations are particular instances of the population we collected: specific instantiations of the population which form our sample. This is sometimes referred as \textbf{\textit{statistical population}}.

A general guideline for a sample is that the sample should be \textbf{\textit{representative}} of the population, where sampling theory itself is another branch worth investigating. The key assumption we make here that the sample we use comes from random sampling such that the properties of a given observation do not affect the probability of it being sampled (randomness) and the properties of one observation do not affect other observations being sampled (independence). A violation is usually the presence of panel data, where time is involved in the scheme.

\subsection{Variables}
\begin{definition}
    \textbf{\textit{Variables}} are mathematical notation of the properties of an observation.
\end{definition} 
We phrase the population as \textbf{\textit{random variable}} to be specific.

Generally, the types of variables are either \textbf{qualitative vs. quantitative} or \textbf{discrete vs. continuous}.
\begin{quote}
    1. Qualitative: represent the qualities of an observation, cannot compare numerically. The only operation is to compare whether two variables are the same or not. These variables can be coded into whole numbers but they still represent qualitative variables. \\
    2. Quantitative: represent quantities of an observation, they are \textbf{rankable}. They have a comparison operation defined for their values. They can come in either \textit{ordinal} or \textit{cardinal} values.
    \begin{quote}
        1. Cardinal: data that have an order \textbf{and} a \textit{relative size} \\
        2. Ordinal: data where only the order matters
    \end{quote}
    3. Discrete: random variables that take on values from within delineated categories, usually represented with integers. They can either finite or infinite. \\
    4. Continuous: random variables that take on an uncountable number of values, between any two values, there is always another value possible.
\end{quote}

Notation-wise, we use capital letters like $X$ to denote random variables. We use lowercase letters $x$ to denote the value of a random variable, \textbf{which in most cases, is a vector of values}. Subscripts $i$ represent each independency, and $t$ represent time. Superscripts means exponentiation.

Another special case of variables is \textbf{\textit{dummy/indicator variables}}.
\begin{quote}
    \textbf{\textit{Dummy or indicator variables}} are variables that are discrete and qualitative that take on two levels $0$ and $1$ to indicate the presence of a certain attribute.
\end{quote}
If a qualitative variable has $k$ levels, then it can be expressed in $k-1$ dummy variables, \textbf{NOT} $k$ dummy variables which falls into the dummy variable trap that suffers from perfect multicollinearity. \\
For a dummy variable,
$$D_i = \begin{cases}
    1 \\
    0
\end{cases}$$
We have,
$$E(D_i) = \sum_d d_i \cdot P(D_i = d_i) = P(D_i = 1)$$
They are important because:
\begin{quote}
    1. flexibly describe qualitative variables \\
    2. treated similarly to cardinal quantitative variables \\
    3. most common type of data in real-world datasets
\end{quote}

\subsection{Parameters and Estimators}
We want to use the \textbf{distribution} of the variables, specifically the \textbf{parameters} of such distributions to answer questions regarding key values or relationships. In this case, 
\begin{definition}
    A \textbf{\textit{sample statistic}} which is used to estimate a specific parameter is called an \textbf{\textit{estimator}}.
\end{definition}
The properties of an estimator usually include:
\begin{quote}
    1. Unbiasedness: the expected value of the estimator is the parameter being estimated, $E(\hat{\mu}) = \mu$ \\
    2. Consistency: As the sample size become larger and closer to the population size, the estimator's value is the parameter being estimated, $\displaystyle \lim_{n \to \infty} \hat{\mu} = \mu$ \\
    3. Efficiency: the estimator has the smallest possible error relative to a particular criterion
\end{quote}

We use a \textbf{Greek letter} to represent a parameter of interest, $\mu, \sigma, \rho$, where $\mu$ is the population mean, $\sigma$ is the population standard deviation, and $\rho$ is the population correlation. \\
We usually use $\hat{}$ to denote an estimator of a certain parameter, like $\hat{\mu}$ for $\mu$. The exceptions include:
\begin{quote}
    1. $\bar{X} = \hat{\mu}_X$ \\
    2. $s = \hat{\sigma}$ \\
    3. $r = \hat{\rho}$
\end{quote}
Subscripts are used when there is ambiguity about the parameter or estimator.

\subsection{Expectation}
Nearly all parameters can be phrased in terms of expectations, specifically:
$$E(X) = \int_x xf(x) \dd x \iff \sum_x x p(x)$$
By definition $E(X) = \mu_X$, if the population mean exists.

A way to form estimators of expected values is via the \textbf{\textit{sample analogue principle}}. The sample analogue of $E(X)$ is:
$$\hat{E}(X) = \frac{1}{n} \sum_{i = 1}^{n} X_i$$

Other important parameters include (co)variance and correlation:
$$C(X, Y) = E((X - E(X))(Y - E(Y)))$$
$$\sigma^2_X = V(X) = C(X, X) = E((X - E(X))^2)$$
$$\rho_{X, Y} = \frac{C(X, Y)}{\sqrt{V(X)V(Y)}}$$
For example, the sample analogue of the variance of $X$ is:
$$\hat{V}(X) = \frac{1}{n} \sum_{i = 1}^{n} (X_i - \bar{X})^2$$

\begin{definition}
    The $k$-\textbf{\textit{th moment}} of the distribution of $X$ is $E(X^k)$
\end{definition}

\subsection{Inference, Sampling Distribution, and Bootstrapping}
The reason why include all these relevant concepts is for statistical inference, where we 
\begin{quote}
    1. have an economic theory we want to test or examine \\
    2. our theory predicts a relationship in the data \\
    3. relate that prediction to statement about a parameter \\
    4. select an estimator of the parameter of interest (e.g. $\hat{\theta}$) \\
    5. perform a test of the statement (e.g. $\hat{\theta} > 0$)
\end{quote}

We want to focus on the \textbf{\textit{sampling distribution}} of an estimator to describe the parameter we are estimating well. The sampling distribution usually has these two properties to adhere to: \textbf{\textit{Law of Large Numbers, and Central Limit Theorem}}.

A technique that can used on samples with small size is \textbf{\textit{bootstrapping}}, where we simulate a large number of possible samples from the original samples. We assume the original sample is the best possible estimator of the population, so we compute our estimator of interest to create an \textbf{\textit{empiral distribution}} analogous to the sampling distribution.

Finally, we can utilize hypothesis tests to see the relationship between the estimator and the parameter of interest. These tests give us robust statistical answers to our research question of interest.

\newpage