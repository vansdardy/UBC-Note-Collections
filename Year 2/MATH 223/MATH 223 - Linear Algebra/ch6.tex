\section{System of Linear Equations, 线性方程组}
\subsection{System of Linear Equations}
Generally, if $A \in M_{m \times n}(\F)$, then we consider
$$Ax = b$$
to represent a system of linear equations where
\begin{align*}
    a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n &= b_1 \\
    a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n &= b_2 \\
    \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n &= b_m
\end{align*}
where
$$A = \begin{pmatrix}
    a_{11} & a_{12} & \dots & a_{1n} \\
    a_{21} & a_{22} & \dots & a_{2n} \\
    \vdots & \vdots &       & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix},
x = \begin{pmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
\end{pmatrix},
b = \begin{pmatrix}
    b_1 \\
    b_2 \\
    \vdots \\
    b_m
\end{pmatrix}$$
If $b_i = 0$, then the system is said to be \textbf{\textit{homogeneous}}（齐次）. \\
Then we define
\begin{definition}
    Given a system of linear equations $Ax = b$, the solution set is
    $$A^{-1}(b) := \{x: Ax = b\}$$
\end{definition}
If we already know $Ax_0 = b$, then every solution to this linear system is of form $x_0 + k$, where $k \in \Ker A$. \\
A useful remark for the solvability of a linear system is that, $Ax=b$ is solvable if and only if
$$\rk A = \rk (A|b)$$

\subsection{Solving Linear Systems, 求解线性方程组}
We apply Gaussian elimination (elementary row transformation) to reduce $A$ to its echelon form, that is for $i^{th}$ row, the leading $1$ should appear at or after $i^{th}$ entry of the row, and all entries before and below the leading $1$s should vanish to $0$. \\
A row-reduced echelon form follows all the rules above while having $0$s above all leading $1$s. \\
If the linear system has a solution, then $b \in \im A$. \\
If the linear system has a UNIQUE solution, then $\Ker A = \{0\}$ and $A$ is injective. \\
If $A$ is a square matrix, then $A$ is surjective, then a solution exists for every $b$. \\
To define a vector space using $Ax = 0$, the vector space is the solution set to this linear system, that is, $\Ker A$. If $A$ is not full rank, then we can express the kernel in parametric form to find bases for the vector space. For example
$$\begin{pmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
\end{pmatrix}
\to
\begin{pmatrix}
    1 & 0 & -1 \\
    0 & 1 & 2 \\
    0 & 0 & 0
\end{pmatrix}$$
Let $z = t$, then $x = t$, and $y = -2t$, then
$$\Ker A = t \begin{pmatrix}
    1 \\
    -2 \\
    1
\end{pmatrix}$$
where $\begin{pmatrix}
    1 \\
    -2 \\
    1
\end{pmatrix}$ is the basis of this vector space. \\
Since row operations preserve $\Ker A$ and $\im A$, we already know the dimension of the kernel is $1$ from above, then by rank-nullity theorem, $\dim (\im A)$. Furthermore, since the columns of $A$ represent the image of the unit vectors, any $2$ vectors can span $\im A$, thus, we have an equation for $\im A$, which is
$$s\begin{pmatrix}
    1 \\
    4 \\
    7
\end{pmatrix} + t \begin{pmatrix}
    2 \\
    5 \\
    8
\end{pmatrix}$$
After Gaussian elimination, we define \textbf{\textit{pivot columns}} to be columns with a leading "1", where \textbf{\textit{non-pivot columns}} to be columns without a leading "1". Each of the non-pivot columns contributes a parameter.

\subsection{Inverting a Matrix, 求逆矩阵}
First, consider an $n \times n$ square matrix $A$, where $Ax = b$.
\begin{quote}
    1. If $\rk A < n$, $A^{-1}$ does not exist. \\
    2. For some $b$, there will be infinitely many solutions, then these $b$ form a linear subspace of $\im A$. \\
    3. For some $b$, there will be no solution, then these $b$ have the property that $b \notin \im A$.
\end{quote}
Since row operations are essentially matrix compositions:
$$\begin{pmatrix}
    0 & 1 \\
    1 & 0
\end{pmatrix}A: \text{swapping rows}$$
$$\begin{pmatrix}
    \lambda & 0 \\
    0 & 1
\end{pmatrix}A: \text{multiplying a row by $\lambda$}$$
$$\begin{pmatrix}
    1 & \lambda \\
    0 & 1
\end{pmatrix}A: \text{Adding multiples of one row to another}$$
Thus, if the row-reduced echelon form of $A$ is equivalent to $\id$, then $A^{-1}$ exists. So $x = A^{-1}b$ from the above case. \\
To find $A^{-1}$, the most direct way is to
$$\begin{pmatrix}
    A & | & \id
\end{pmatrix} \xrightarrow{Gaussian Elimination} \begin{pmatrix}
    \id & | & A^{-1}
\end{pmatrix}$$
\newpage