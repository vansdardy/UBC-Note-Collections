\section{Euclidean Vector Space, 欧几里得向量空间}
\subsection{Inner Product, 内积}
Let $V$ be a vector space, and we define an extra structure - inner product, a Euclidean space:
\begin{definition}
    An inner product on $V$ over $\R$ is a function:
    $$(*, *) : V \times V \to \R$$
    which satisfies:
    \begin{quote}
        1. Bi-linearity
        $$(\lambda_1x_1 + \lambda_2x_2, y) = \lambda_1(x_1, y) + \lambda_2(x_2, y)$$
        $$(x, \lambda_1y_1 + \lambda_2y_2) = \lambda_1(x, y_1) + \lambda_2(x, y_2)$$
        2. Symmetry
        $$(x, y) = (y, x)$$
        3. Positive-definite
        $$\forall x \in V, (x, x) \ge 0, x = 0 \iff (x, x) = 0$$
    \end{quote}
\end{definition}
For a complex vector space, we can ask for positive-definite, but then we need to modify bi-linearity and symmetry to Hermitian form, for example, if $x, y \in \C^n, x = (z_1,  \dots, z_n), y = (w_1, \dots, w_n)$, we can have $(x, y) = z_1 \bar{w_1} + \dots + z_n \bar{w_n}$. \\
Any finite-dimensional vector space $V$ over $\R$ has $\infty$ many inner products. \\
Thus, a \textbf{\textit{Euclidean space}} is defined to be a vector space with some inner product on such a vector space. \\
This is useful to define Euclidean geometry, where it involves distances and angles.
\begin{definition}
    A \textbf{\textit{Euclidean norm}}（欧几里得范数） is defined to be
    $$\|x\| := \sqrt{(x, x)}$$
\end{definition}
Then,
\begin{definition}
    If we consider $x, y \in V$, the angle between them to be $\alpha$, then by Law of Cosines, we have
    $$\cos{\alpha} = \frac{(x, y)}{\|x\|\|y\|}$$
\end{definition}
In a Euclidean space, we define the unit sphere $S$ to be a set of vectors where
$$S := \{x \in V: \|x\| = 1\}$$
For function vector space, for example, all real continuous functions on $[0, 1]$, we can define its inner product to be
$$(f, g) := \int_0^1 f(x)g(x) \dd x$$
For complex-value functions, we can use Hermitian form of some
$$(f, g) = \int f(x)\overline{g(x)} \dd x$$
\subsubsection{Properties of a Norm, 范数的性质}
Norms do not have to always come from an inner product, as axiomatically, we only require a norm to satisfy three conditions: 
\begin{quote}
    1. $\|x\| = 0 \iff x = 0$, otherwise $\|x\| > 0$ \\
    2. $\|\lambda x\| = |\lambda| \|x\|$ \\
    3. $\|x+y\| \le \|x\| + \|y\|$
\end{quote}

\subsection{Cauchy-Schwarz Inequality, 柯西——施瓦茨不等式}
\begin{theorem}
    Cauchy-Schwarz Inequality states that
    $$|(x, y)| \le \|x\| \|y\|$$
\end{theorem}
In continuous functions on $[a, b]$, this applies to be
$$(\int_a^b f(x)g(x) \dd x)^2 \le \int_a^b f(x)^2 \dd x \int_a^b g(x)^2 \dd x$$
In real numbers, we then have
$$(\sum_{i=1}^n x_iy_i)^2 \le (\sum_{i=1}^n x_i)^2 (\sum_{i=1}^n y_i)^2$$
\subsection{Gram-Schmidt Orthonormalization, 格拉姆——施密特正交化}
The convenience of a Euclidean space is that it gives an orthonormal basis which is of many uses.
\begin{definition}
    Let $V$ be a Euclidean vector space, and for some vectors $v_1, \dots, v_n \in V$, if $\forall i, j = 1, \dots, n, \|v_i\| = 1 \land (i \ne j) \implies (v_i, v_j) = 0$, then $v_1, \dots, v_n$ are orthonormal.
\end{definition}
\begin{lemma}
    Any collection of orthonormal vectors is linearly independent
\end{lemma}
Thus, if $\{v_1, \dots, v_n\}$ is an orthonormal basis for Euclidean vector space $V$, if we express $v = \lambda_1v_1 + \dots + \lambda_n v_n \in V$, then $\lambda_i = (v, v_i)$
From a random basis, to find an orthonormal basis of the same vector space, we follow this process
\begin{theorem}
    Given a random basis $\{v_1, \dots, v_n\}$: \\
    1. Normalize $v_1$ to $\Tilde{v_1}$ where $\|\Tilde{v_1}\| = 1$ \\
    2. For $k = 1, \dots, n-1$, we have
    $$\Tilde{v_{k+1}} = \frac{v_{k+1} - \sum_{i=1}^k(v_{k+1}, \Tilde{v_i})\Tilde{v_i}}{\|v_{k+1} - \sum_{i=1}^k(v_{k+1}, \Tilde{v_i})\Tilde{v_i}\|}$$
    Then, $\{\Tilde{v_1}, \dots, \Tilde{v_n}\}$ is an orthonormal basis.
\end{theorem}

\subsubsection{Projections, 投影}
Let $V = U \oplus W$, then $\forall v \in V, \exists ! v = u + w, u \in U \land w \in W$. \\
Given a linear subspace $U$, unless it is over a finite field, there are $\infty$ many projections, the best of which is the orthogonal complement.
\begin{definition}
    An orthogonal complement of $U$ is defined to be
    $$U^\perp := \{u' \in V: \forall u \in U, (u, u') = 0\}$$
    We have $U \oplus U^\perp = V$
\end{definition}
Orthogonal projectors are thus projectors that project $U$ along $U^\perp$ to $U$.

\subsubsection{Relation to Matrix, 与矩阵的关系}
Consider $A = (a_{ij}) \in M_{m \times n}$, then $A \begin{pmatrix}
    x_1 \\
    \vdots \\
    x_n
\end{pmatrix}$ can be interpreted as a dot product. \\
Let $r_i = \begin{pmatrix}
    a_{i1} & \dots & a_{in}
\end{pmatrix}$, and $r_i \cdot x$ is the dot product, then
$$x \in \Ker A \iff r_i \cdot x = 0 \iff x \in L(r_1, \dots, r_m)^\perp$$
This tells us, the column of space of $A$ is $\im A$, and the \textbf{\textit{row space}}（行空间）of $A$ is $\Ker(A)^\perp$. \\
And given that $V = \Ker (A) \oplus \Ker(A)^\perp$, we have
\begin{align*}
    \dim(\Ker(A)^\perp) &= \dim V - \dim(\Ker A) \\
    \rk A &= \dim V - \dim(\Ker A)
\end{align*}
That is: \textbf{column rank = row rank}

\subsection{Orthogonal Matrix, 正交矩阵}
\begin{definition}
    Linear map $A: V \to W$ is an \textbf{\textit{isometry}}（等距同构）if
    $$(Av_1, Av_2) = (v_1, v_2)$$
    where $V, W$ are Euclidean vector spaces, and $v_1, v_2 \in V$. \\
    Usually, it means $A$ is surjective.
\end{definition}
Such isometries take any orthonormal basis to an orthonormal basis, therefore, if $A$ is an endomorphism, and let $c_k$ denote the $k^{th}$ column of $A$, then with
$$c_i \cdot c_j = \delta_{ij} = \begin{dcases}
        1 & i = j \\
        0 & i \ne j
    \end{dcases}$$
We further will have $A^tA = \id$
More generally,
\begin{theorem}
    Let $A: V \to W$, $\dim V = \dim W$, be an isometry, then \\
    $\iff$ $(Av_1, Av_2) = (v_1, v_2)$ \\
    $\iff$ orthonormal basis of $V$ $\to$ orthonormal basis of $W$ \\
    $\iff$ columns of $A$ is the orthonormal basis of $W$ \\
    $\iff$ $A^tA = \id \iff A^t = A^{-1} \iff \det A = \pm 1$ \\
    $\iff$ $AA^t = \id \to$ rows of $A$ form an orthonormal basis \\
    $\to$ $\det A_{ij} = \pm a_{ij}$
\end{theorem}
Orthonormal transformations of $V$ form a group called $O(V)$ or $O_n(\R)$, and those special orthonormal transformations' group is denoted as $SO(V)$ or $SO_3(\R)$.
\newpage